{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99605951-f251-499d-85a2-e935cbaea82f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### In this photo, I tried to show the difference between LLM and RALM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b69f4-b36f-44df-b555-d7b50b17af5a",
   "metadata": {},
   "source": [
    "![image](./1722417000990.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7488f-cfc7-458c-9dff-35de70b709a9",
   "metadata": {},
   "source": [
    "### Sample usage RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22ec5b-94c1-41c5-976d-2c070465e774",
   "metadata": {},
   "source": [
    "```bash\n",
    "uv add scikit-learn transformers numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba317e82-ef0c-4c02-8ff5-c610e17e230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a55d332-be52-4c61-adbc-853401503080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd779bf-2f87-4d7f-a7f7-0b5d47dcd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e8baec-ae1e-47f3-9f54-dc01e1ddf2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a3b2d4-8cee-4c9c-9d08-953f1c3078b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up a small document collection\n",
    "documents = [\n",
    "    \"Retrieval-Augmented Generation (RAG) combines retrieval and generation for improved AI responses.\",\n",
    "    \"RAG models retrieve relevant information before generating answers to questions.\",\n",
    "    \"The retrieval component helps RAG models access knowledge beyond their training data.\",\n",
    "    \"Unlike standard language models, RAG can cite sources for the information it provides.\",\n",
    "    \"RAG architecture helps reduce hallucinations by grounding responses in retrieved documents.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b523ea-c172-4dcb-90a3-a0bd4bc7db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create a simple retrieval system using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "def retrieve_docs(query, top_k=2):\n",
    "    # Convert query to vector\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    similarities = cosine_similarity(query_vector, document_vectors)[0]\n",
    "    \n",
    "    # Get indices of top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Return selected documents and their scores\n",
    "    return [(documents[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038d9679-920e-4425-8b1d-3d63dc50c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up a simple generative model (using a pre-trained model)\n",
    "# Note: In a real implementation, you would use a proper LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded67647-0f7f-45af-8af0-64507a767987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set the pad token for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_response(context, query):\n",
    "    # Format the prompt\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize with proper attention mask\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512  # Set an appropriate max_length for your input\n",
    "    )\n",
    "    \n",
    "    # Generate with proper pad_token_id\n",
    "    output = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,  # Add the attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,   # Explicitly set pad_token_id\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        # temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Decode and return response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ad4bdb-25d7-4263-bded-5b8895ef9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: RAG pipeline that combines retrieval and generation\n",
    "def rag_pipeline(query):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_docs(query)\n",
    "    \n",
    "    # Format retrieved documents as context\n",
    "    context = \" \".join([doc for doc, score in retrieved_docs])\n",
    "    \n",
    "    # Generate response based on retrieved context\n",
    "    response = generate_response(context, query)\n",
    "    \n",
    "    # Format final output with citations\n",
    "    final_response = f\"{response}\\n\\nSources:\\n\"\n",
    "    for i, (doc, score) in enumerate(retrieved_docs):\n",
    "        final_response += f\"[{i+1}] {doc} (Score: {score:.2f})\\n\"\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872a816c-23c0-4be2-8d42-2315fd9ac09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"How does RAG reduce hallucinations?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3370a8-bf7c-427e-b4de-d1b8fec0e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsen/frr/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = rag_pipeline(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30edac3e-6594-4bf3-b9b9-4cdb8896de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: RAG architecture helps reduce hallucinations by grounding responses in retrieved documents. RAG models retrieve relevant information before generating answers to questions.\n",
      "\n",
      "Question: How does RAG reduce hallucinations?\n",
      "\n",
      "Answer: RAG reduces hallucinations by grounding responses in retrieved documents.\n",
      "\n",
      "Question: How does RAG reduce hallucinations?\n",
      "\n",
      "Answer: RAG reduces hallucinations by grounding responses in retrieved documents.\n",
      "\n",
      "Question: How does RAG reduce hallucinations?\n",
      "\n",
      "Answer: RAG reduces hallucinations by grounding responses in retrieved documents.\n",
      "\n",
      "Question: How does RAG reduce hallucinations?\n",
      "\n",
      "Answer: RAG reduces hallucinations by grounding responses in retrieved documents.\n",
      "\n",
      "Question: How does RAG reduce hallucinations?\n",
      "\n",
      "Answer: RAG reduces hallucinations\n",
      "\n",
      "Sources:\n",
      "[1] RAG architecture helps reduce hallucinations by grounding responses in retrieved documents. (Score: 0.48)\n",
      "[2] RAG models retrieve relevant information before generating answers to questions. (Score: 0.05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c37b0d-2898-48e2-8668-9f1748c07e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
